version: "3.9"
services:
  integrated_llm:
    build: .
    container_name: integrated_llm
    volumes:
      - ./:/home/scott/goose-ai:rw
      - ./ollama_data:/home/scott/.ollama:rw
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_CUDA=1
      - NVIDIA_VISIBLE_DEVICES=all
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
      limits:
        memory: 32G